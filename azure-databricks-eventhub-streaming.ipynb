{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5580ce51",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================\n",
    "# Real-Time Streaming with Azure Databricks + Event Hubs (Medallion)\n",
    "# =============================================================\n",
    "\n",
    "try:\n",
    "    dbutils  # type: ignore\n",
    "except NameError:\n",
    "    class _DB:\n",
    "        class widgets:\n",
    "            _vals = {}\n",
    "            @staticmethod\n",
    "            def text(name, defaultValue=\"\", label=\"\"): _DB.widgets._vals[name] = defaultValue\n",
    "            @staticmethod\n",
    "            def dropdown(name, defaultValue, choices, label=\"\"): _DB.widgets._vals[name] = defaultValue\n",
    "            @staticmethod\n",
    "            def get(name): return _DB.widgets._vals.get(name, \"\")\n",
    "            @staticmethod\n",
    "            def removeAll(): _DB.widgets._vals.clear()\n",
    "        class fs:\n",
    "            @staticmethod\n",
    "            def ls(path): return []\n",
    "    dbutils = _DB()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b12f5549",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =====================\n",
    "# 1) PARAMETERS\n",
    "# =====================\n",
    "dbutils.widgets.removeAll()\n",
    "\n",
    "# Event Hubs\n",
    "dbutils.widgets.text(\"eh_connection\", \"REPLACE_WITH_EVENT_HUBS_CONNECTION_STRING\", \"Event Hubs Connection String\")\n",
    "dbutils.widgets.text(\"eh_consumer_group\", \"$Default\", \"Consumer Group\")\n",
    "dbutils.widgets.dropdown(\"eh_start_position\", \"@latest\", [\"@latest\",\"@earliest\"], \"Starting Position\")\n",
    "\n",
    "# Storage (ADLS Gen2 abfss:// URIs or mounted paths)\n",
    "dbutils.widgets.text(\"bronze_path\", \"abfss://datalake@REPLACE.dfs.core.windows.net/bronze/iot_events\", \"Bronze Path\")\n",
    "dbutils.widgets.text(\"silver_path\", \"abfss://datalake@REPLACE.dfs.core.windows.net/silver/iot_events\", \"Silver Path\")\n",
    "dbutils.widgets.text(\"gold_path\",   \"abfss://datalake@REPLACE.dfs.core.windows.net/gold/iot_kpis\",   \"Gold Path\")\n",
    "\n",
    "# Checkpoints\n",
    "dbutils.widgets.text(\"ckp_bronze\", \"abfss://datalake@REPLACE.dfs.core.windows.net/_checkpoints/bronze/iot_events\", \"Bronze Checkpoint\")\n",
    "dbutils.widgets.text(\"ckp_silver\", \"abfss://datalake@REPLACE.dfs.core.windows.net/_checkpoints/silver/iot_events\", \"Silver Checkpoint\")\n",
    "dbutils.widgets.text(\"ckp_gold\",   \"abfss://datalake@REPLACE.dfs.core.windows.net/_checkpoints/gold/iot_kpis\",   \"Gold Checkpoint\")\n",
    "\n",
    "# Table names (optional)\n",
    "dbutils.widgets.text(\"tbl_bronze\", \"iot_raw_bronze\", \"Bronze Table\")\n",
    "dbutils.widgets.text(\"tbl_silver\", \"iot_clean_silver\", \"Silver Table\")\n",
    "dbutils.widgets.text(\"tbl_gold\",   \"iot_kpi_gold\", \"Gold Table\")\n",
    "\n",
    "params = {k: dbutils.widgets.get(k) for k in [\n",
    "    \"eh_connection\",\"eh_consumer_group\",\"eh_start_position\",\n",
    "    \"bronze_path\",\"silver_path\",\"gold_path\",\n",
    "    \"ckp_bronze\",\"ckp_silver\",\"ckp_gold\",\n",
    "    \"tbl_bronze\",\"tbl_silver\",\"tbl_gold\"\n",
    "]}\n",
    "print(\"PARAMETERS:\", {k: (\"***\" if \"connection\" in k else v) for k,v in params.items()})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28c04b16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =====================\n",
    "# 2) IMPORTS & SCHEMAS\n",
    "# =====================\n",
    "from pyspark.sql.functions import col, from_json, to_timestamp, current_timestamp, window, expr, coalesce\n",
    "from pyspark.sql.types import StructType, StructField, StringType, DoubleType, LongType\n",
    "\n",
    "# Expected event body (UTF-8 JSON):\n",
    "# { \"deviceId\":\"dev-1\", \"site\":\"ams\", \"temperature\":22.5, \"humidity\":0.45, \"ts\":1724175600 }\n",
    "raw_schema = StructType([\n",
    "    StructField(\"deviceId\", StringType()),\n",
    "    StructField(\"site\", StringType()),\n",
    "    StructField(\"temperature\", DoubleType()),\n",
    "    StructField(\"humidity\", DoubleType()),\n",
    "    StructField(\"ts\", LongType())\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afccf0a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =====================\n",
    "# 3) EVENT HUBS → BRONZE (DELTA)\n",
    "# =====================\n",
    "eh_conf = {\n",
    "    \"eventhubs.connectionString\": params[\"eh_connection\"],\n",
    "    \"eventhubs.consumerGroup\": params[\"eh_consumer_group\"],\n",
    "    \"eventhubs.startingPosition\": params[\"eh_start_position\"]\n",
    "}\n",
    "\n",
    "raw_stream = (spark.readStream\n",
    "    .format(\"eventhubs\")\n",
    "    .options(**eh_conf)\n",
    "    .load()\n",
    ")\n",
    "\n",
    "bronze_df = (raw_stream\n",
    "    .withColumn(\"body_str\", col(\"body\").cast(\"string\"))\n",
    "    .withColumn(\"ingest_ts\", current_timestamp())\n",
    ")\n",
    "\n",
    "bronze_writer = (bronze_df.writeStream\n",
    "    .format(\"delta\")\n",
    "    .outputMode(\"append\")\n",
    "    .option(\"checkpointLocation\", params[\"ckp_bronze\"])\n",
    "    .option(\"path\", params[\"bronze_path\"])\n",
    "    .trigger(processingTime=\"10 seconds\")\n",
    ")\n",
    "\n",
    "# bronze_query = bronze_writer.start()\n",
    "# display(bronze_query)\n",
    "print(\"Bronze stream defined. Uncomment start() to run.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72541a19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =====================\n",
    "# 4) BRONZE → SILVER (PARSE, CLEANSE, DEDUP)\n",
    "# =====================\n",
    "bronze_read = (spark.readStream\n",
    "    .format(\"delta\")\n",
    "    .load(params[\"bronze_path\"])\n",
    ")\n",
    "\n",
    "parsed = (bronze_read\n",
    "    .withColumn(\"json\", from_json(col(\"body_str\"), raw_schema))\n",
    "    .select(\n",
    "        col(\"json.deviceId\").alias(\"device_id\"),\n",
    "        col(\"json.site\").alias(\"site\"),\n",
    "        col(\"json.temperature\").alias(\"temperature\"),\n",
    "        col(\"json.humidity\").alias(\"humidity\"),\n",
    "        # If ts is epoch seconds, to_timestamp will handle Unix epoch. Adjust if ISO string.\n",
    "        to_timestamp(col(\"json.ts\")).alias(\"event_time\")\n",
    "    )\n",
    ")\n",
    "\n",
    "clean = (parsed\n",
    "    .filter(col(\"device_id\").isNotNull())\n",
    "    .withWatermark(\"event_time\", \"5 minutes\")\n",
    "    .dropDuplicates([\"device_id\",\"event_time\"])\n",
    ")\n",
    "\n",
    "silver_writer = (clean.writeStream\n",
    "    .format(\"delta\")\n",
    "    .outputMode(\"append\")\n",
    "    .option(\"checkpointLocation\", params[\"ckp_silver\"])\n",
    "    .option(\"path\", params[\"silver_path\"])\n",
    "    .trigger(processingTime=\"10 seconds\")\n",
    ")\n",
    "\n",
    "# silver_query = silver_writer.start()\n",
    "# display(silver_query)\n",
    "print(\"Silver stream defined. Uncomment start() to run.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bec34f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =====================\n",
    "# 5) SILVER → GOLD (WINDOWED KPIs)\n",
    "# =====================\n",
    "silver_read = (spark.readStream\n",
    "    .format(\"delta\")\n",
    "    .load(params[\"silver_path\"])\n",
    ")\n",
    "\n",
    "kpi = (silver_read\n",
    "    .withWatermark(\"event_time\", \"10 minutes\")\n",
    "    .groupBy(\n",
    "        window(col(\"event_time\"), \"1 minute\"),\n",
    "        col(\"site\"),\n",
    "        col(\"device_id\")\n",
    "    )\n",
    "    .agg(\n",
    "        expr(\"avg(temperature) as avg_temperature\"),\n",
    "        expr(\"avg(humidity) as avg_humidity\"),\n",
    "        expr(\"count(*) as reading_count\")\n",
    "    )\n",
    "    .selectExpr(\n",
    "        \"window.start as window_start\",\n",
    "        \"window.end as window_end\",\n",
    "        \"site\",\"device_id\",\"avg_temperature\",\"avg_humidity\",\"reading_count\"\n",
    "    )\n",
    ")\n",
    "\n",
    "gold_writer = (kpi.writeStream\n",
    "    .format(\"delta\")\n",
    "    .outputMode(\"complete\")\n",
    "    .option(\"checkpointLocation\", params[\"ckp_gold\"])\n",
    "    .option(\"path\", params[\"gold_path\"])\n",
    "    .trigger(processingTime=\"1 minute\")\n",
    ")\n",
    "\n",
    "# gold_query = gold_writer.start()\n",
    "# display(gold_query)\n",
    "print(\"Gold stream defined. Uncomment start() to run.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fe1a4db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =====================\n",
    "# 6) TABLE REGISTRATION (OPTIONAL)\n",
    "# =====================\n",
    "spark.sql(f\"CREATE TABLE IF NOT EXISTS {params['tbl_bronze']} USING DELTA LOCATION '{params['bronze_path']}'\")\n",
    "spark.sql(f\"CREATE TABLE IF NOT EXISTS {params['tbl_silver']} USING DELTA LOCATION '{params['silver_path']}'\")\n",
    "spark.sql(f\"CREATE TABLE IF NOT EXISTS {params['tbl_gold']}   USING DELTA LOCATION '{params['gold_path']}'\")\n",
    "print(\"Tables created if metastore/unity catalog is configured.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7989d275",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =====================\n",
    "# 7) OPTIONAL EVENT GENERATOR → EVENT HUBS\n",
    "# =====================\n",
    "# %pip install azure-eventhub  (on Databricks)\n",
    "import json, random, time\n",
    "try:\n",
    "    from azure.eventhub import EventHubProducerClient, EventData\n",
    "    HAVE_EH=True\n",
    "except Exception:\n",
    "    HAVE_EH=False\n",
    "    print(\"Install azure-eventhub to enable generator.\")\n",
    "\n",
    "eh_conn = params[\"eh_connection\"]\n",
    "batches = 3\n",
    "events_per_batch = 50\n",
    "\n",
    "if HAVE_EH and eh_conn.startswith(\"Endpoint\"):\n",
    "    producer = EventHubProducerClient.from_connection_string(conn_str=eh_conn)\n",
    "    for b in range(batches):\n",
    "        batch = producer.create_batch()\n",
    "        for i in range(events_per_batch):\n",
    "            payload = {\n",
    "                \"deviceId\": f\"dev-{random.randint(1,5)}\",\n",
    "                \"site\": random.choice([\"ams\",\"utrecht\",\"rotterdam\"]),\n",
    "                \"temperature\": round(random.uniform(18.0, 30.0),2),\n",
    "                \"humidity\": round(random.uniform(0.3, 0.8),2),\n",
    "                \"ts\": int(time.time())\n",
    "            }\n",
    "            batch.add(EventData(json.dumps(payload)))\n",
    "        producer.send_batch(batch)\n",
    "        print(f\"Sent batch {b+1}/{batches}\")\n",
    "        time.sleep(1)\n",
    "    producer.close()\n",
    "else:\n",
    "    print(\"Generator not executed. Provide a valid Event Hubs connection string including EntityPath.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41c8f40c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =====================\n",
    "# 8) EXAMPLE ANALYTICS (SQL VIA PYTHON)\n",
    "# =====================\n",
    "spark.sql(f\"SELECT * FROM {params['tbl_gold']} ORDER BY window_end DESC LIMIT 20\").show(truncate=False)\n",
    "\n",
    "spark.sql(f'''\n",
    "SELECT device_id, site, avg_temperature, avg_humidity, reading_count, window_start, window_end\n",
    "FROM {params['tbl_gold']}\n",
    "WHERE window_end > now() - INTERVAL 30 MINUTES\n",
    "ORDER BY avg_temperature DESC, reading_count DESC\n",
    "LIMIT 20\n",
    "''').show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d138611",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =====================\n",
    "# 9) STREAM HELPERS\n",
    "# =====================\n",
    "def stop_all_streams():\n",
    "    for q in spark.streams.active:\n",
    "        try:\n",
    "            print(\"Stopping:\", q.name or q.id)\n",
    "            q.stop()\n",
    "        except Exception as e:\n",
    "            print(\"Error:\", e)\n",
    "\n",
    "print(\"Call stop_all_streams() to stop all active streams.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.x"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
