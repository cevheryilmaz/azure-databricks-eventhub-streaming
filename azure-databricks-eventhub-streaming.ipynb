{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2738350a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Databricks Real-Time Streaming with Azure Event Hubs → Delta Lake (Bronze/Silver/Gold)\n",
    "# Generated: 2025-08-20 21:09:06\n",
    "# Notes:\n",
    "# - This notebook is code-only (no markdown cells) for clean GitHub rendering.\n",
    "# - Replace the placeholders below with your actual secrets and paths before running on Databricks.\n",
    "# - Designed for Databricks Runtime 12+ with Delta enabled.\n",
    "\n",
    "# Utility: detect if running on Databricks and provide lightweight shims for dbutils to allow local static analysis.\n",
    "try:\n",
    "    dbutils  # type: ignore\n",
    "except NameError:\n",
    "    class _DBUtilsShim:\n",
    "        class widgets:\n",
    "            _vals = {}\n",
    "            @staticmethod\n",
    "            def text(name, defaultValue=\"\", label=\"\"):\n",
    "                _DBUtilsShim.widgets._vals[name] = defaultValue\n",
    "            @staticmethod\n",
    "            def dropdown(name, defaultValue, choices, label=\"\"):\n",
    "                _DBUtilsShim.widgets._vals[name] = defaultValue\n",
    "            @staticmethod\n",
    "            def get(name):\n",
    "                return _DBUtilsShim.widgets._vals.get(name, \"\")\n",
    "            @staticmethod\n",
    "            def removeAll():\n",
    "                _DBUtilsShim.widgets._vals.clear()\n",
    "        class fs:\n",
    "            @staticmethod\n",
    "            def ls(path): return []\n",
    "    dbutils = _DBUtilsShim()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68627e80",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------------------\n",
    "# 1) PARAMETERS (Widgets)\n",
    "# ------------------------------\n",
    "dbutils.widgets.removeAll()\n",
    "\n",
    "# Event Hubs\n",
    "dbutils.widgets.text(\"eh_connection\", \"REPLACE_WITH_EVENT_HUBS_CONNECTION_STRING\", \"Event Hubs Connection String\")\n",
    "dbutils.widgets.text(\"eh_consumer_group\", \"$Default\", \"Consumer Group\")\n",
    "dbutils.widgets.dropdown(\"eh_start_position\", \"@latest\", [\"@latest\", \"@earliest\"], \"Start Position\")\n",
    "\n",
    "# Storage paths (ADLS Gen2 mounted paths or abfss:// URIs)\n",
    "dbutils.widgets.text(\"bronze_path\", \"abfss://datalake@REPLACE.dfs.core.windows.net/bronze/iot_events\", \"Bronze Path\")\n",
    "dbutils.widgets.text(\"silver_path\", \"abfss://datalake@REPLACE.dfs.core.windows.net/silver/iot_events\", \"Silver Path\")\n",
    "dbutils.widgets.text(\"gold_path\",   \"abfss://datalake@REPLACE.dfs.core.windows.net/gold/iot_kpis\",   \"Gold Path\")\n",
    "\n",
    "# Checkpoints (one per stream sink)\n",
    "dbutils.widgets.text(\"ckp_bronze\", \"abfss://datalake@REPLACE.dfs.core.windows.net/_checkpoints/bronze/iot_events\", \"Bronze Checkpoint\")\n",
    "dbutils.widgets.text(\"ckp_silver\", \"abfss://datalake@REPLACE.dfs.core.windows.net/_checkpoints/silver/iot_events\", \"Silver Checkpoint\")\n",
    "dbutils.widgets.text(\"ckp_gold\",   \"abfss://datalake@REPLACE.dfs.core.windows.net/_checkpoints/gold/iot_kpis\",   \"Gold Checkpoint\")\n",
    "\n",
    "# Table names (optional if you want Hive Metastore/Unity Catalog)\n",
    "dbutils.widgets.text(\"tbl_bronze\", \"iot_raw_bronze\", \"Table Bronze\")\n",
    "dbutils.widgets.text(\"tbl_silver\", \"iot_clean_silver\", \"Table Silver\")\n",
    "dbutils.widgets.text(\"tbl_gold\",   \"iot_kpi_gold\", \"Table Gold\")\n",
    "\n",
    "params = {k: dbutils.widgets.get(k) for k in [\n",
    "    \"eh_connection\",\"eh_consumer_group\",\"eh_start_position\",\n",
    "    \"bronze_path\",\"silver_path\",\"gold_path\",\n",
    "    \"ckp_bronze\",\"ckp_silver\",\"ckp_gold\",\n",
    "    \"tbl_bronze\",\"tbl_silver\",\"tbl_gold\"\n",
    "]}\n",
    "print(\"PARAMETERS LOADED:\", {k: (\"***\" if \"connection\" in k else v) for k,v in params.items()})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0adc8d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------------------\n",
    "# 2) IMPORTS & SCHEMA\n",
    "# ------------------------------\n",
    "from pyspark.sql.functions import col, from_json, to_timestamp, window, current_timestamp, expr, coalesce, lit\n",
    "from pyspark.sql.types import StructType, StructField, StringType, DoubleType, LongType, TimestampType\n",
    "\n",
    "# Expected JSON payload from Event Hubs body (utf-8 string):\n",
    "# {\n",
    "#   \"deviceId\": \"dev-123\",\n",
    "#   \"site\": \"ams\",\n",
    "#   \"temperature\": 21.3,\n",
    "#   \"humidity\": 0.42,\n",
    "#   \"ts\": 1718035200  # epoch seconds (or provide an ISO string and adjust parsing)\n",
    "# }\n",
    "raw_schema = StructType([\n",
    "    StructField(\"deviceId\", StringType()),\n",
    "    StructField(\"site\", StringType()),\n",
    "    StructField(\"temperature\", DoubleType()),\n",
    "    StructField(\"humidity\", DoubleType()),\n",
    "    StructField(\"ts\", LongType())\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b46a6604",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------------------\n",
    "# 3) READ STREAM FROM EVENT HUBS → BRONZE (DELTA)\n",
    "# ------------------------------\n",
    "# Connector config (Event Hubs Spark connector)\n",
    "# See: https://learn.microsoft.com/azure/event-hubs/event-hubs-spark-structured-streaming\n",
    "\n",
    "eh_conf = {\n",
    "    \"eventhubs.connectionString\": params[\"eh_connection\"],\n",
    "    \"eventhubs.consumerGroup\": params[\"eh_consumer_group\"],\n",
    "    \"eventhubs.startingPosition\": params[\"eh_start_position\"]\n",
    "}\n",
    "\n",
    "# Read stream\n",
    "raw_stream = (spark.readStream\n",
    "    .format(\"eventhubs\")\n",
    "    .options(**eh_conf)\n",
    "    .load()\n",
    ")\n",
    "\n",
    "# The 'body' column is binary. Convert to string for JSON parsing and keep metadata for traceability.\n",
    "bronze_df = (raw_stream\n",
    "    .withColumn(\"body_str\", col(\"body\").cast(\"string\"))\n",
    "    .withColumn(\"ingest_ts\", current_timestamp())\n",
    ")\n",
    "\n",
    "# Write to Bronze as Delta (append-only, raw immutable)\n",
    "bronze_writer = (bronze_df.writeStream\n",
    "    .format(\"delta\")\n",
    "    .outputMode(\"append\")\n",
    "    .option(\"checkpointLocation\", params[\"ckp_bronze\"])\n",
    "    .option(\"path\", params[\"bronze_path\"])\n",
    "    .trigger(processingTime=\"10 seconds\")\n",
    ")\n",
    "\n",
    "# Start the Bronze stream (uncomment in Databricks)\n",
    "# bronze_query = bronze_writer.start()\n",
    "# display(bronze_query)\n",
    "print(\"Bronze stream writer defined. Uncomment start() to run on Databricks.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73513717",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------------------\n",
    "# 4) TRANSFORM BRONZE → SILVER (PARSE JSON, CLEANSE, DEDUP)\n",
    "# ------------------------------\n",
    "bronze_read = (spark.readStream\n",
    "    .format(\"delta\")\n",
    "    .load(params[\"bronze_path\"])\n",
    ")\n",
    "\n",
    "parsed = (bronze_read\n",
    "    .withColumn(\"json\", from_json(col(\"body_str\"), raw_schema))\n",
    "    .select(\n",
    "        col(\"json.deviceId\").alias(\"device_id\"),\n",
    "        col(\"json.site\").alias(\"site\"),\n",
    "        col(\"json.temperature\").alias(\"temperature\"),\n",
    "        col(\"json.humidity\").alias(\"humidity\"),\n",
    "        # prefer event ts if present else use enqueueTime or ingest_ts\n",
    "        to_timestamp(col(\"json.ts\")).alias(\"event_time_ts\")  # if ts is epoch seconds, use: to_timestamp(col(\"json.ts\"))\n",
    "    )\n",
    "    .withColumn(\"event_time\", coalesce(col(\"event_time_ts\"), current_timestamp()))\n",
    "    .drop(\"event_time_ts\")\n",
    ")\n",
    "\n",
    "clean = (parsed\n",
    "    .filter(col(\"device_id\").isNotNull())\n",
    "    .withWatermark(\"event_time\", \"5 minutes\")\n",
    "    # idempotent dedup if device + event_time used as composite key\n",
    "    .dropDuplicates([\"device_id\",\"event_time\"])\n",
    ")\n",
    "\n",
    "silver_writer = (clean.writeStream\n",
    "    .format(\"delta\")\n",
    "    .outputMode(\"append\")\n",
    "    .option(\"checkpointLocation\", params[\"ckp_silver\"])\n",
    "    .option(\"path\", params[\"silver_path\"])\n",
    "    .trigger(processingTime=\"10 seconds\")\n",
    ")\n",
    "\n",
    "# silver_query = silver_writer.start()\n",
    "# display(silver_query)\n",
    "print(\"Silver stream writer defined. Uncomment start() to run on Databricks.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "069ab4eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------------------\n",
    "# 5) AGGREGATE SILVER → GOLD (WINDOWED KPIs)\n",
    "# ------------------------------\n",
    "silver_read = (spark.readStream\n",
    "    .format(\"delta\")\n",
    "    .load(params[\"silver_path\"])\n",
    ")\n",
    "\n",
    "# Example KPIs: per device and 1-minute window → avg temperature, avg humidity, count\n",
    "kpi = (silver_read\n",
    "    .withWatermark(\"event_time\", \"10 minutes\")\n",
    "    .groupBy(\n",
    "        window(col(\"event_time\"), \"1 minute\"),\n",
    "        col(\"site\"),\n",
    "        col(\"device_id\")\n",
    "    )\n",
    "    .agg(\n",
    "        expr(\"avg(temperature) as avg_temperature\"),\n",
    "        expr(\"avg(humidity) as avg_humidity\"),\n",
    "        expr(\"count(*) as reading_count\")\n",
    "    )\n",
    "    .select(\n",
    "        col(\"window.start\").alias(\"window_start\"),\n",
    "        col(\"window.end\").alias(\"window_end\"),\n",
    "        \"site\", \"device_id\", \"avg_temperature\", \"avg_humidity\", \"reading_count\"\n",
    "    )\n",
    ")\n",
    "\n",
    "gold_writer = (kpi.writeStream\n",
    "    .format(\"delta\")\n",
    "    .outputMode(\"complete\")\n",
    "    .option(\"checkpointLocation\", params[\"ckp_gold\"])\n",
    "    .option(\"path\", params[\"gold_path\"])\n",
    "    .trigger(processingTime=\"1 minute\")\n",
    ")\n",
    "\n",
    "# gold_query = gold_writer.start()\n",
    "# display(gold_query)\n",
    "print(\"Gold stream writer defined. Uncomment start() to run on Databricks.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d82e9278",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------------------\n",
    "# 6) OPTIONAL: REGISTER DELTA TABLES\n",
    "# ------------------------------\n",
    "spark.sql(f\"CREATE TABLE IF NOT EXISTS {params['tbl_bronze']} USING DELTA LOCATION '{params['bronze_path']}'\")\n",
    "spark.sql(f\"CREATE TABLE IF NOT EXISTS {params['tbl_silver']} USING DELTA LOCATION '{params['silver_path']}'\")\n",
    "spark.sql(f\"CREATE TABLE IF NOT EXISTS {params['tbl_gold']}   USING DELTA LOCATION '{params['gold_path']}'\")\n",
    "print(\"Tables created (if permissions/catalog configured).\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d23cd917",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------------------\n",
    "# 7) OPTIONAL: DATA GENERATOR → EVENT HUBS (RUN AS A ONE-OFF JOB)\n",
    "# ------------------------------\n",
    "# Use only if you want to push sample events. Requires azure-eventhub Python package.\n",
    "# On Databricks: %pip install azure-eventhub\n",
    "# Then fill in the connection string including EntityPath.\n",
    "# WARNING: This cell sends data to your Event Hub when executed.\n",
    "\n",
    "import json, random, time\n",
    "from datetime import datetime, timezone\n",
    "\n",
    "try:\n",
    "    from azure.eventhub import EventHubProducerClient, EventData\n",
    "    HAVE_EH = True\n",
    "except Exception as e:\n",
    "    HAVE_EH = False\n",
    "    print(\"azure-eventhub not installed. To enable generator: %pip install azure-eventhub\")\n",
    "\n",
    "eh_conn = params[\"eh_connection\"]  # must include EntityPath\n",
    "num_batches = 5\n",
    "events_per_batch = 100\n",
    "\n",
    "if HAVE_EH and eh_conn.startswith(\"Endpoint\"):\n",
    "    producer = EventHubProducerClient.from_connection_string(conn_str=eh_conn)\n",
    "    for b in range(num_batches):\n",
    "        batch = producer.create_batch()\n",
    "        for i in range(events_per_batch):\n",
    "            payload = {\n",
    "                \"deviceId\": f\"dev-{random.randint(1,5)}\",\n",
    "                \"site\": random.choice([\"ams\",\"utrecht\",\"rotterdam\"]),\n",
    "                \"temperature\": round(random.uniform(18.0, 30.0), 2),\n",
    "                \"humidity\": round(random.uniform(0.3, 0.8), 2),\n",
    "                \"ts\": int(time.time())\n",
    "            }\n",
    "            batch.add(EventData(json.dumps(payload)))\n",
    "        producer.send_batch(batch)\n",
    "        print(f\"Sent batch {b+1}/{num_batches}\")\n",
    "        time.sleep(1)\n",
    "    producer.close()\n",
    "else:\n",
    "    print(\"Generator not executed (missing azure-eventhub or invalid connection string).\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfc5af3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------------------\n",
    "# 8) EXAMPLE ANALYTICS (SQL)\n",
    "# ------------------------------\n",
    "spark.sql(f\"SELECT * FROM {params['tbl_gold']} ORDER BY window_end DESC LIMIT 20\").show(truncate=False)\n",
    "\n",
    "# Top hot devices (last 30 minutes)\n",
    "spark.sql(f'''\n",
    "SELECT device_id, site, avg_temperature, reading_count, window_start, window_end\n",
    "FROM {params['tbl_gold']}\n",
    "WHERE window_end > now() - INTERVAL 30 MINUTES\n",
    "ORDER BY avg_temperature DESC, reading_count DESC\n",
    "LIMIT 20\n",
    "''').show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3609e534",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------------------\n",
    "# 9) STREAM MANAGEMENT HELPERS\n",
    "# ------------------------------\n",
    "def stop_all_streams():\n",
    "    for q in spark.streams.active:\n",
    "        try:\n",
    "            print(\"Stopping:\", q.name or q.id)\n",
    "            q.stop()\n",
    "        except Exception as e:\n",
    "            print(\"Error stopping:\", e)\n",
    "\n",
    "print(\"Use stop_all_streams() to terminate streams when needed.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.x"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
